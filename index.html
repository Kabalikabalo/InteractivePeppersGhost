<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>Gesture Gallery (GestureRecognizer, Minimal)</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    html, body { height: 100%; margin: 0; background: #000; overflow: hidden; }
    /* Only the image is visible */
    #ghost {
      position: absolute; inset: 0; margin: auto;
      max-width: 90vw; max-height: 90vh; object-fit: contain;
      transform: scale(1) translateZ(0); transition: transform 0.12s ease;
      user-select: none; pointer-events: none; filter: drop-shadow(0 0 30px rgba(0,0,0,0.6));
    }
    /* Hidden video for the recognizer */
    #cam { display: none; }
    /* One-tap starter (required by some browsers for cam permissions) */
    #tap {
      position: absolute; inset: 0; display: grid; place-items: center;
      color: #888; font: 600 14px system-ui; background: transparent; cursor: pointer;
    }
    #tap.hide { display: none; }
  </style>
</head>
<body>
  <img id="ghost" alt="image" />
  <video id="cam" autoplay playsinline muted></video>
  <div id="tap">tap/click to start camera</div>

  <!-- Use MediaPipe Tasks Vision (GestureRecognizer) -->
  <script type="module">
    import {
      GestureRecognizer,
      FilesetResolver
    } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.14";

    // Public images that read well on black
    const IMAGES = [
      "https://upload.wikimedia.org/wikipedia/commons/9/97/The_Earth_seen_from_Apollo_17.jpg",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/3/33/NGC2207%2BIC2163.jpg/640px-NGC2207%2BIC2163.jpg",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/f/f5/Venus_colour.png/640px-Venus_colour.png",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/5/5a/NGC6543.jpg/640px-NGC6543.jpg",
      "https://upload.wikimedia.org/wikipedia/commons/thumb/0/00/Crab_Nebula.jpg/640px-Crab_Nebula.jpg"
    ];

    const imgEl  = document.getElementById('ghost');
    const videoEl = document.getElementById('cam');
    const tapEl   = document.getElementById('tap');

    let idx = 0;
    let scale = 1;
    const SCALE_MIN = 0.4, SCALE_MAX = 2.8, SCALE_STEP = 0.02;

    function clamp(v,a,b){ return Math.max(a, Math.min(b, v)); }
    function setScale(s){ scale = clamp(s, SCALE_MIN, SCALE_MAX); imgEl.style.transform = `scale(${scale}) translateZ(0)`; }

    // preload with onerror-skip
    function showImage(i, depth=0){
      if (depth > IMAGES.length) return; // stop if all fail
      const safe = ((i % IMAGES.length) + IMAGES.length) % IMAGES.length;
      idx = safe;
      const probe = new Image();
      probe.onload  = () => { imgEl.src = IMAGES[idx]; };
      probe.onerror = () => { showImage(idx + 1, depth + 1); }; // skip broken
      probe.src = IMAGES[idx];
    }
    showImage(idx);
    setScale(1);

    // Debounce "Pointing_Up" â†’ next image
    let lastPointAt = 0;
    const POINT_COOLDOWN_MS = 700;

    // Map gesture labels -> actions
    function handleGesture(label){
      if (label === "Open_Palm") {
        setScale(scale + SCALE_STEP);
      } else if (label === "Closed_Fist") {
        setScale(scale - SCALE_STEP);
      } else if (label === "Thumb_Up") {
        const now = performance.now();
        if (now - lastPointAt > POINT_COOLDOWN_MS) {
          lastPointAt = now;
          showImage(idx + 1);
        }
      }
    }

    let started = false;
    async function start(){
      if (started) return; started = true;
      tapEl.classList.add('hide');

      // Prepare camera
      const stream = await navigator.mediaDevices.getUserMedia({ video: { width: 640, height: 480 }});
      videoEl.srcObject = stream;
      await videoEl.play();

      // Load WASM core and the gesture model
      const fileset = await FilesetResolver.forVisionTasks(
        "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.14/wasm"
      );
      const recognizer = await GestureRecognizer.createFromOptions(fileset, {
        baseOptions: {
          // Official hosted model (float16) from Google
          modelAssetPath: "https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task"
        },
        runningMode: "VIDEO",
        numHands: 1
      });

      // Video loop
      const loop = () => {
        const now = performance.now();
        const result = recognizer.recognizeForVideo(videoEl, now);
        // result.gestures: Array per hand, with classification list
        if (result && result.gestures && result.gestures.length > 0) {
          const top = result.gestures[0][0]; // highest score classification for first hand
          if (top && top.categoryName && top.score > 0.6) {
            handleGesture(top.categoryName);
          }
        }
        requestAnimationFrame(loop);
      };
      requestAnimationFrame(loop);
    }

    // Start on user interaction (required by some browsers)
    tapEl.addEventListener('click', start);
    imgEl.addEventListener('click', start);
    window.addEventListener('keydown', (e)=>{
      if (e.key === 'ArrowRight') showImage(idx + 1);
      if (e.key === '+') setScale(scale + 0.05);
      if (e.key === '-') setScale(scale - 0.05);
      start();
    });
  </script>
</body>
</html>
