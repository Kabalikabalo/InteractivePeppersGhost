<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8" />
<title>Hand? YES / NO</title>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<style>
  html,body { height:100%; margin:0; background:#000; overflow:hidden; }
  #status {
    position:fixed; inset:0; display:grid; place-items:center;
    font:900 clamp(48px, 20vw, 200px)/1 system-ui, -apple-system, Segoe UI, Roboto, sans-serif;
    letter-spacing:.02em; color:#f33; user-select:none;
  }
  #status.yes { color:#3f6; }
  #start { position:fixed; inset:0; display:grid; place-items:center;
    color:#bbb; font:600 14px system-ui; cursor:pointer; }
  #start.hide { display:none; }
  video#cam { display:none; }
</style>
</head>
<body>
  <div id="status" aria-live="polite">NO</div>
  <div id="start">tap/click to start</div>
  <video id="cam" autoplay playsinline muted></video>

<script type="module">
  import { HandLandmarker, FilesetResolver }
    from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.14";

  const statusEl = document.getElementById('status');
  const startEl  = document.getElementById('start');
  const cam      = document.getElementById('cam');

  // simple hysteresis so the text doesn't flicker
  const HOLD_MS = 300;
  let lastSeenAt = 0;
  let showingYes = false;

  function setStatus(hasHand){
    const now = performance.now();
    if (hasHand) lastSeenAt = now;
    const shouldShowYes = hasHand || (now - lastSeenAt < HOLD_MS);
    if (shouldShowYes !== showingYes) {
      showingYes = shouldShowYes;
      statusEl.textContent = showingYes ? 'YES' : 'NO';
      statusEl.classList.toggle('yes', showingYes);
    }
  }

  const TARGET_FPS = 15;                   // throttle inference for mobile
  const INTERVAL   = 1000 / TARGET_FPS;
  let lastInfer = 0, started = false, pageHidden = false;

  document.addEventListener('visibilitychange', ()=>{ pageHidden = document.hidden; });

  async function start(){
    if (started) return; started = true; startEl.classList.add('hide');

    // small camera feed (hidden)
    const stream = await navigator.mediaDevices.getUserMedia({
      video:{
        width:{ideal:320, max:320},
        height:{ideal:240, max:240},
        frameRate:{ideal:15, max:15},
        facingMode:{ideal:'user'} // try 'environment' if you prefer rear cam
      }
    });
    cam.srcObject = stream; await cam.play();

    // load hand landmark model
    const fileset = await FilesetResolver.forVisionTasks(
      "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.14/wasm"
    );
    const handLM = await HandLandmarker.createFromOptions(fileset, {
      baseOptions:{
        modelAssetPath:
          "https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/latest/hand_landmarker.task"
      },
      runningMode: "VIDEO",
      numHands: 1,
      minHandDetectionConfidence: 0.35,
      minHandPresenceConfidence: 0.3,
      minTrackingConfidence: 0.2
    });

    // quick re-acquire helper (IMAGE mode once) if tracking drops
    async function reacquireOnce(){
      await handLM.setOptions({ runningMode: "IMAGE" });
      const r = await handLM.detect(cam);
      await handLM.setOptions({ runningMode: "VIDEO" });
      return r;
    }

    let misses = 0;

    const loop = async ()=>{
      const now = performance.now();
      if (!pageHidden && now - lastInfer >= INTERVAL){
        lastInfer = now;
        let r = handLM.detectForVideo(cam, now);

        if (!r?.landmarks?.length){
          misses++;
          if (misses % 6 === 0) r = await reacquireOnce(); // ~400ms cadence
        } else {
          misses = 0;
        }

        setStatus(!!r?.landmarks?.length);
      }
      requestAnimationFrame(loop);
    };
    requestAnimationFrame(loop);

    // clean up camera on unload
    window.addEventListener('pagehide', ()=> stream.getTracks().forEach(t=>t.stop()));
  }

  startEl.addEventListener('click', start);
  window.addEventListener('keydown', start);
  document.body.addEventListener('click', start);
</script>
</body>
</html>
